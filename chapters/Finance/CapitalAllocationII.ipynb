{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e2cc237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas_datareader.data as web\n",
    "\n",
    "def get_factors(factors='CAPM',freq='daily'):   \n",
    "    \n",
    "    if freq=='monthly':\n",
    "        freq_label=''\n",
    "    else:\n",
    "        freq_label='_'+freq\n",
    "\n",
    "\n",
    "    if factors=='CAPM':\n",
    "        fama_french = web.DataReader(\"F-F_Research_Data_Factors\"+freq_label, \"famafrench\",start=\"1921-01-01\")\n",
    "        daily_data = fama_french[0]\n",
    "    \n",
    "     \n",
    "        df_factor = daily_data[['RF','Mkt-RF']] \n",
    "    elif factors=='FF3':\n",
    "        fama_french = web.DataReader(\"F-F_Research_Data_Factors\"+freq_label, \"famafrench\",start=\"1921-01-01\")\n",
    "        daily_data = fama_french[0]\n",
    "\n",
    "        df_factor = daily_data[['RF','Mkt-RF','SMB','HML']]\n",
    "    elif factors=='FF5':\n",
    "\n",
    "        fama_french = web.DataReader(\"F-F_Research_Data_Factors\"+freq_label, \"famafrench\",start=\"1921-01-01\")\n",
    "        daily_data = fama_french[0]\n",
    "\n",
    "        df_factor = daily_data[['RF','Mkt-RF','SMB','HML']]\n",
    "        fama_french2 = web.DataReader(\"F-F_Research_Data_5_Factors_2x3\"+freq_label, \"famafrench\",start=\"1921-01-01\")\n",
    "        daily_data2 = fama_french2[0]\n",
    "\n",
    "        df_factor2 = daily_data2[['RMW','CMA']]\n",
    "        df_factor=df_factor.merge(df_factor2,on='Date',how='outer')    \n",
    "        \n",
    "    else:\n",
    "        fama_french = web.DataReader(\"F-F_Research_Data_Factors\"+freq_label, \"famafrench\",start=\"1921-01-01\")\n",
    "        daily_data = fama_french[0]\n",
    "\n",
    "        df_factor = daily_data[['RF','Mkt-RF','SMB','HML']]\n",
    "        fama_french2 = web.DataReader(\"F-F_Research_Data_5_Factors_2x3\"+freq_label, \"famafrench\",start=\"1921-01-01\")\n",
    "        daily_data2 = fama_french2[0]\n",
    "\n",
    "        df_factor2 = daily_data2[['RMW','CMA']]\n",
    "        df_factor=df_factor.merge(df_factor2,on='Date',how='outer')   \n",
    "        fama_french = web.DataReader(\"F-F_Momentum_Factor\"+freq_label, \"famafrench\",start=\"1921-01-01\")\n",
    "        df_factor=df_factor.merge(fama_french[0],on='Date')\n",
    "        df_factor.columns=['RF','Mkt-RF','SMB','HML','RMW','CMA','MOM']    \n",
    "    if freq=='monthly':\n",
    "        df_factor.index = pd.to_datetime(df_factor.index.to_timestamp())\n",
    "    else:\n",
    "        df_factor.index = pd.to_datetime(df_factor.index)\n",
    "        \n",
    "\n",
    "\n",
    "    return df_factor/100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade7e942",
   "metadata": {},
   "source": [
    "# Capital Allocation II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4beeb9",
   "metadata": {},
   "source": [
    "We learned how to maximize Sharpe Ratio if we are sure about the moments\n",
    "\n",
    "$$E[R^e]Var(R^e)^{-1}$$\n",
    "\n",
    "- Investment managers rarely apply the Markowitz framework blindly\n",
    "\n",
    "* Why not used more often? \n",
    "\n",
    "* It requires specifying expected returns for entire universe of assets\n",
    "\n",
    "* Mean-variance optimization weights heavily on assets that have extreme average returns in the sample and with attractive covariance properties, i.e., high correlation with asset yielding slightly different return\n",
    "\n",
    "\n",
    "\n",
    "**In reality** knowing the true moments of the return distribution is HARD as we discussed in the last few weeks\n",
    "\n",
    "- We will start by using \"In Sample\" means and variances\n",
    "- By construction weight $W=w E[R^e]\\Sigma_{R}^{-1} $ will deliver the Highest in sample Sharpe Ratio. It is just math!\n",
    "- The issue is to what extent these in sample moments are informative about the forward looking moments\n",
    "* What problems that might cause?\n",
    "\n",
    "* The key problem is that we are dealing with a limited sample of data\n",
    "\n",
    "* We are interested in finding the \"ex-ante\" tangency portfolio\n",
    "\n",
    "* But the basic MV analysis identifies the ex-post tangency portfolio, i.e. a portfolio that did well in the sample, but it can be a statistical fluke\n",
    "\n",
    "* The issue is that we do not observe the true population expectation and covariances, but only have a sample estimate\n",
    "\n",
    "* Whatever your research process, you are likely to end up quite uncertain about the alphas of your trading strategies\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf4572c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "* We will start by using what we learned and measuring  the uncertainty regarding our risk premia estimates for each asset\n",
    "\n",
    "* We will then show how the weights change as we pertubate these estimates in a way that is consistent with the amount of uncertainty in our estimates\n",
    "\n",
    "* We will then show how sensitive the benefits of optimization are\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6640d64",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "We will use 6 classic academic strategies: Market, Size, Value, Profitability, Investment, and Momentum.\n",
    "\n",
    "These six factors are particularly popular both in the industry and in the academic community\n",
    "\n",
    "Our portfolio problem will be to find the maximum SR combination out of these strategies\n",
    "\n",
    "In a couple weeks we will learn how to construct these strategies from scratch\n",
    "\n",
    "A few of them have ETFS that aim to replicate them, which potentially allow retail investors to get exposure to them cheaply and also industry people to easily hedge their factor exposures. (We will investigate carefully to what extent these ETFs do a good job..coming soon in a theater near you!) \n",
    "\n",
    "For now we will use this data to take the perspective of an investor that does not differentiate across the factors and simply cares for the final SR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c177d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alan.moreira\\Anaconda3\\lib\\site-packages\\pandas_datareader\\famafrench.py:114: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  df = read_csv(StringIO(\"Date\" + src[start:]), **params)\n",
      "c:\\Users\\alan.moreira\\Anaconda3\\lib\\site-packages\\pandas_datareader\\famafrench.py:114: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  df = read_csv(StringIO(\"Date\" + src[start:]), **params)\n",
      "c:\\Users\\alan.moreira\\Anaconda3\\lib\\site-packages\\pandas_datareader\\famafrench.py:114: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  df = read_csv(StringIO(\"Date\" + src[start:]), **params)\n",
      "c:\\Users\\alan.moreira\\Anaconda3\\lib\\site-packages\\pandas_datareader\\famafrench.py:114: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  df = read_csv(StringIO(\"Date\" + src[start:]), **params)\n",
      "c:\\Users\\alan.moreira\\Anaconda3\\lib\\site-packages\\pandas_datareader\\famafrench.py:114: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  df = read_csv(StringIO(\"Date\" + src[start:]), **params)\n",
      "c:\\Users\\alan.moreira\\Anaconda3\\lib\\site-packages\\pandas_datareader\\famafrench.py:114: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  df = read_csv(StringIO(\"Date\" + src[start:]), **params)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RF</th>\n",
       "      <th>Mkt-RF</th>\n",
       "      <th>SMB</th>\n",
       "      <th>HML</th>\n",
       "      <th>RMW</th>\n",
       "      <th>CMA</th>\n",
       "      <th>MOM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-02-01</th>\n",
       "      <td>0.0033</td>\n",
       "      <td>-0.0244</td>\n",
       "      <td>-0.0579</td>\n",
       "      <td>0.0491</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0306</td>\n",
       "      <td>-0.0081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-01</th>\n",
       "      <td>0.0034</td>\n",
       "      <td>-0.0639</td>\n",
       "      <td>-0.0276</td>\n",
       "      <td>0.0290</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>-0.0047</td>\n",
       "      <td>-0.0284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-01</th>\n",
       "      <td>0.0035</td>\n",
       "      <td>-0.0084</td>\n",
       "      <td>-0.0059</td>\n",
       "      <td>-0.0340</td>\n",
       "      <td>-0.0285</td>\n",
       "      <td>-0.0267</td>\n",
       "      <td>0.0497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-01</th>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0606</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>-0.0288</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0251</td>\n",
       "      <td>0.0221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-01</th>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0486</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>-0.0160</td>\n",
       "      <td>-0.0319</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>-0.0265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                RF  Mkt-RF     SMB     HML     RMW     CMA     MOM\n",
       "Date                                                              \n",
       "2025-02-01  0.0033 -0.0244 -0.0579  0.0491  0.0110  0.0306 -0.0081\n",
       "2025-03-01  0.0034 -0.0639 -0.0276  0.0290  0.0211 -0.0047 -0.0284\n",
       "2025-04-01  0.0035 -0.0084 -0.0059 -0.0340 -0.0285 -0.0267  0.0497\n",
       "2025-05-01  0.0038  0.0606  0.0070 -0.0288  0.0126  0.0251  0.0221\n",
       "2025-06-01  0.0034  0.0486  0.0083 -0.0160 -0.0319  0.0145 -0.0265"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ff6=get_factors('ff6',freq='monthly').dropna()\n",
    "df_ff6.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e2575ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.54580584, 1.68734284, 0.60113252, 4.15873819, 4.59852945,\n",
       "       2.05279602])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I will drop the risk-free asset and allocate only across excess returns\n",
    "df=df_ff6['1965':].drop(columns=['RF'])\n",
    "\n",
    "W=df.mean()@ np.linalg.inv(df.cov())\n",
    "# lets leverage it to have a yearly vol of 16%\n",
    "\n",
    "W = W / np.sqrt(W @ df.cov() @ W.T) * 0.16\n",
    "W "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf267a52",
   "metadata": {},
   "source": [
    "Now what? What should we look at?\n",
    "\n",
    "- In sample Vol\n",
    "- In sample SR\n",
    "- Out of sample Vol\n",
    "- Out of Sample SR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1facc08f",
   "metadata": {},
   "source": [
    "**Optimal Weights sensitivity to uncertainty**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cfcaf3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samplemean</th>\n",
       "      <th>Mkt-RF-2std</th>\n",
       "      <th>SMB-2std</th>\n",
       "      <th>HML-2std</th>\n",
       "      <th>RMW-2std</th>\n",
       "      <th>CMA-2std</th>\n",
       "      <th>MOM-2std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mkt-RF</th>\n",
       "      <td>0.734911</td>\n",
       "      <td>0.520451</td>\n",
       "      <td>0.822647</td>\n",
       "      <td>0.727309</td>\n",
       "      <td>0.786210</td>\n",
       "      <td>0.693475</td>\n",
       "      <td>0.764402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMB</th>\n",
       "      <td>0.487094</td>\n",
       "      <td>0.656413</td>\n",
       "      <td>0.065126</td>\n",
       "      <td>0.477350</td>\n",
       "      <td>0.366378</td>\n",
       "      <td>0.479881</td>\n",
       "      <td>0.544483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HML</th>\n",
       "      <td>0.173532</td>\n",
       "      <td>0.217508</td>\n",
       "      <td>0.182509</td>\n",
       "      <td>-0.499773</td>\n",
       "      <td>0.277651</td>\n",
       "      <td>0.702197</td>\n",
       "      <td>0.047779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMW</th>\n",
       "      <td>1.200524</td>\n",
       "      <td>1.320447</td>\n",
       "      <td>1.025730</td>\n",
       "      <td>1.271688</td>\n",
       "      <td>0.712395</td>\n",
       "      <td>1.169612</td>\n",
       "      <td>1.375193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CMA</th>\n",
       "      <td>1.327481</td>\n",
       "      <td>1.286719</td>\n",
       "      <td>1.311413</td>\n",
       "      <td>1.960872</td>\n",
       "      <td>1.325569</td>\n",
       "      <td>0.318267</td>\n",
       "      <td>1.566690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MOM</th>\n",
       "      <td>0.592591</td>\n",
       "      <td>0.630909</td>\n",
       "      <td>0.621360</td>\n",
       "      <td>0.488529</td>\n",
       "      <td>0.685222</td>\n",
       "      <td>0.693325</td>\n",
       "      <td>0.361953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        samplemean  Mkt-RF-2std  SMB-2std  HML-2std  RMW-2std  CMA-2std  \\\n",
       "Mkt-RF    0.734911     0.520451  0.822647  0.727309  0.786210  0.693475   \n",
       "SMB       0.487094     0.656413  0.065126  0.477350  0.366378  0.479881   \n",
       "HML       0.173532     0.217508  0.182509 -0.499773  0.277651  0.702197   \n",
       "RMW       1.200524     1.320447  1.025730  1.271688  0.712395  1.169612   \n",
       "CMA       1.327481     1.286719  1.311413  1.960872  1.325569  0.318267   \n",
       "MOM       0.592591     0.630909  0.621360  0.488529  0.685222  0.693325   \n",
       "\n",
       "        MOM-2std  \n",
       "Mkt-RF  0.764402  \n",
       "SMB     0.544483  \n",
       "HML     0.047779  \n",
       "RMW     1.375193  \n",
       "CMA     1.566690  \n",
       "MOM     0.361953  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets look at what happens with weights and Sharpes as we manipulate the mean of the market return by one standard deviation\n",
    "\n",
    "\n",
    "\n",
    "Wmve=pd.DataFrame([],index=df.columns)\n",
    "\n",
    "ERe=df.mean()\n",
    "CovRe=df.cov()\n",
    "T=df.shape[0]\n",
    "\n",
    "for asset in df.columns:\n",
    "    manipulation=2\n",
    "    mu=df[asset].mean()\n",
    "    # the standard deviation of the sample mean is the standard deviation of the asset divided by the square root of the number of observations\n",
    "    musigma=df[asset].std()/T**0.5\n",
    "    # I am creating a copy of the average return vector so I can manipulate it below \n",
    "    ERemsig=ERe.copy()\n",
    "\n",
    "    ERemsig[asset]=mu-manipulation*musigma\n",
    "\n",
    "    # mve for sample mean\n",
    "\n",
    "    W=np.linalg.inv(CovRe) @ ERe\n",
    "    Wmve['samplemean'] =W / np.sqrt(W @ CovRe @ W.T) * 0.16/12**0.5\n",
    "    # mve for perturbed mean\n",
    "    W =np.linalg.inv(CovRe) @ ERemsig\n",
    "    # lets leverage it to have a yearly vol of 16%  similar to the market\n",
    "    # the 12**0.5 is due to the fact that the data is monthly\n",
    "    Wmve[asset+'-'+str(manipulation)+'std'] = W / np.sqrt(W @ CovRe @ W.T) * 0.16/12**0.5\n",
    "\n",
    "Wmve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7197f9e7",
   "metadata": {},
   "source": [
    "I am here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d325d45f",
   "metadata": {},
   "source": [
    "**Performance sensitivity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "59abed42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>samplemean</th>\n",
       "      <td>1.179649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mkt-RF-2std</th>\n",
       "      <td>1.150683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMB-2std</th>\n",
       "      <td>1.142918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HML-2std</th>\n",
       "      <td>1.124095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMW-2std</th>\n",
       "      <td>1.147552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CMA-2std</th>\n",
       "      <td>1.118807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MOM-2std</th>\n",
       "      <td>1.151143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   SR\n",
       "samplemean   1.179649\n",
       "Mkt-RF-2std  1.150683\n",
       "SMB-2std     1.142918\n",
       "HML-2std     1.124095\n",
       "RMW-2std     1.147552\n",
       "CMA-2std     1.118807\n",
       "MOM-2std     1.151143"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SR=pd.DataFrame([],index=Wmve.columns)\n",
    "for col in Wmve.columns:\n",
    "    SR.loc[col,'SR'] =Wmve[col] @ ERe / np.sqrt(Wmve[col] @ CovRe @ Wmve[col].T) *12**0.5\n",
    "\n",
    "    \n",
    "SR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b532555b",
   "metadata": {},
   "source": [
    "## Bet sizing under uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec45b6d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The industry developed many Ad-hoc approaches for **bet sizing**  exactly to deal with the fact that Mean-variance optimization is quite sensitive to inputs\n",
    "\n",
    "\n",
    "\n",
    "**Bet sizing** is one of the great skills in a portfolio manager because it requires instinct for uncertainty \n",
    "\n",
    "I show some of the different approaches below\n",
    "\n",
    "Here I am applying them to the problem of combining \"alpha\" strategies, i.e. after you took out obvious sources of co-movement\n",
    "\n",
    "(w is a scalar that controls the overall size of the portfolio )\n",
    "\n",
    "- **Mean-variance** rule (the one we looked so far):\n",
    "  \n",
    "$$W_i=w \\frac{\\alpha_i}{\\sigma_{\\epsilon,i}^2}$$\n",
    "\n",
    "- **1/N** rule: ignore the magnitude of the alpha and simply bet on the direction of your idea\n",
    "\n",
    "$$W_i=\\frac{1}{N}\\left((\\alpha_i>0)-(\\alpha_i<0)\\right)$$\n",
    "\n",
    "> this is good if you have good hunches for mispricing, but you don't get the magnitudes quite right\n",
    "\n",
    "- **Proportional** rule: Buy/sell proportional to the alpha\n",
    "\n",
    "$$W_i=w \\alpha_i$$\n",
    "\n",
    "> This ignore variance information and might be relevant if cannot take leverage\n",
    "\n",
    "- **Risky-parity** rule: assumes the Appraisal ratio of your different ideas are the same \n",
    "  \n",
    "  $$\\frac{\\alpha_i}{\\sigma_i}=\\frac{\\alpha_j}{\\sigma_j}$$\n",
    "\n",
    "  If you apply this to the MV rule you get\n",
    "\n",
    "\n",
    "$$W_i=w \\frac{1}{\\sigma_{\\epsilon,i}}$$\n",
    "\n",
    "which essentially mean you allocate the same vol for each strategy\n",
    "\n",
    "$$W_i\\sigma_{\\epsilon,i}=W_j\\sigma_{\\epsilon,j}$$\n",
    "\n",
    "- **Minimum-Variance** rule: \n",
    "\n",
    "$$W_i=w \\frac{1}{\\sigma_{\\epsilon,i}^2}$$\n",
    "\n",
    "> This assumes alphas are all the same and focus on using the information in the variance matrix to boost the Sharpe ratio\n",
    "> When applying this you want to make flip the strategy to make sure they are all have positive alphas. For example, if the idea is that AI is overvalued jsut make the trading strategy to be short AI stocks\n",
    "  \n",
    "\n",
    "- **Variance shrinkage** rule: you shrink the variance-covariance matrix towards a particular value\n",
    "\n",
    "$$W_i=w \\frac{\\alpha_i}{\\sigma_{\\epsilon,i}^2(1-\\tau)+\\tau\\sigma_{shrink}^2}$$\n",
    "\n",
    "  - where $\\tau\\in[0,1]$ is the shrinkage factor\n",
    "\n",
    "> This makes the variance-covariance matrix behaved and less likely to have very specific correlation patterns\n",
    "> It is good idea to do that in general--so to some extent this is really the MV approach with a better variance estimator\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545466af",
   "metadata": {},
   "source": [
    "Some examples you should play with\n",
    "\n",
    "\n",
    "> ### Stop and Practice\n",
    "> Solve for optimal weights, maximum Sharpe ratio\n",
    "> \n",
    "> Target vol of 10% annualized\n",
    ">\n",
    "> 1. same alpha, same betas, same idio vol\n",
    "> 2. different alpha, same beta , same idio vol\n",
    "> 3. same alpha, different betas, same idio vol\n",
    "> 4. same alpha, same betas, different idio vol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cafd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.array([0.3,0.2,0.1,0.05])\n",
    "B=np.array([1,2,0.5,-0.5])\n",
    "Sigmae=np.diag([0.4,0.4,0.4,0.4])**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9521f91b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    ">#### Alert\n",
    "\n",
    ">This will be stylized in the sense that we will use in sample moments, so we cannot really compare Sharpe ratios\n",
    ">\n",
    ">Why not? Because by construction the Mean-variance will always beat everyone else.\n",
    ">\n",
    ">We use these alternative methods exactly because the in sample moments are often not great guide for the forward looking moments we care about, so a in-sample comparison does not reflect the reality of trading that requires use past information to trade.\n",
    ">\n",
    "> We will later discuss how to make this comparison ( it is not rocket science: divide your sample in estimation and testing samples!)\n",
    "\n",
    "\n",
    "1. We will target Total portfolio with volatility 10%\n",
    "2. We will get the factors and focus on the sample that we have all the factors available\n",
    "3. We will estimate out single-factor models for these factors and build our factor model matrixes\n",
    "4. We will be imposing that the residuals are uncorrelated\n",
    "   1. This is never true in a particular sample\n",
    "   2. If you have a good enough factor model, you impose it because whatever correlation is in the sample is noise (it is testable!)\n",
    "   3. Here this is a toy exercise and these correlations are likely real (for example, mom and HML have shown to be correlated in many different situations)\n",
    "5. We will  apply the different rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b2f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RF</th>\n",
       "      <th>Mkt-RF</th>\n",
       "      <th>SMB</th>\n",
       "      <th>HML</th>\n",
       "      <th>RMW</th>\n",
       "      <th>CMA</th>\n",
       "      <th>MOM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-08-01</th>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>-0.0355</td>\n",
       "      <td>-0.0113</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-01</th>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>-0.0017</td>\n",
       "      <td>-0.0259</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>-0.0026</td>\n",
       "      <td>-0.0060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-01</th>\n",
       "      <td>0.0039</td>\n",
       "      <td>-0.0097</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>-0.0138</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-01</th>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0651</td>\n",
       "      <td>0.0463</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>-0.0262</td>\n",
       "      <td>-0.0217</td>\n",
       "      <td>0.0090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-01</th>\n",
       "      <td>0.0037</td>\n",
       "      <td>-0.0317</td>\n",
       "      <td>-0.0273</td>\n",
       "      <td>-0.0295</td>\n",
       "      <td>0.0182</td>\n",
       "      <td>-0.0110</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                RF  Mkt-RF     SMB     HML     RMW     CMA     MOM\n",
       "Date                                                              \n",
       "2024-08-01  0.0048  0.0161 -0.0355 -0.0113  0.0085  0.0086  0.0479\n",
       "2024-09-01  0.0040  0.0174 -0.0017 -0.0259  0.0004 -0.0026 -0.0060\n",
       "2024-10-01  0.0039 -0.0097 -0.0101  0.0089 -0.0138  0.0103  0.0287\n",
       "2024-11-01  0.0040  0.0651  0.0463 -0.0005 -0.0262 -0.0217  0.0090\n",
       "2024-12-01  0.0037 -0.0317 -0.0273 -0.0295  0.0182 -0.0110  0.0005"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_ff6=get_factors('ff6',freq='monthly').dropna()\n",
    "df_ff6.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fae7e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alan.Moreira\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: [0.00570763 0.04279558 0.04072549 0.04250479 0.08432384]\n",
      "Beta: [ 0.20237897 -0.13569538 -0.09332071 -0.16729257 -0.15829589]\n",
      "Sigma_e: [[0.01021215 0.         0.         0.         0.        ]\n",
      " [0.         0.01031929 0.         0.         0.        ]\n",
      " [0.         0.         0.00568454 0.         0.        ]\n",
      " [0.         0.         0.         0.00447722 0.        ]\n",
      " [0.         0.         0.         0.         0.02048366]]\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Define the factors and the market factor\n",
    "factors = ['SMB', 'HML', 'RMW', 'CMA', 'MOM']\n",
    "market_factor = 'Mkt-RF'\n",
    "\n",
    "# Initialize lists to store the results\n",
    "Alpha = []\n",
    "Beta = []\n",
    "residuals = []\n",
    "Alpha_se = []\n",
    "# Run univariate regressions\n",
    "for factor in factors:\n",
    "    X = sm.add_constant(df_ff6[market_factor])\n",
    "    y = df_ff6[factor]\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    Alpha.append(model.params['const'])\n",
    "    Beta.append(model.params[market_factor])\n",
    "    residuals.append(model.resid)\n",
    "\n",
    "# Convert Alpha and Beta to numpy arrays\n",
    "Alpha = np.array(Alpha)\n",
    "Beta = np.array(Beta)\n",
    "\n",
    "# Calculate the variance-covariance matrix of the residuals\n",
    "#under the assumption that the residuals are uncorrelated ( they are not!)\n",
    "\n",
    "residuals_matrix = np.vstack(residuals).T\n",
    "Sigma_e = np.diag(np.diag(np.cov(residuals_matrix.T)))\n",
    "\n",
    "# Display the results\n",
    "print(\"Alpha:\", Alpha*12)\n",
    "print(\"Beta:\", Beta)\n",
    "print(\"Sigma_e:\", Sigma_e*12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e404fd",
   "metadata": {},
   "source": [
    "Mean Variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81828352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55890596 4.14714181 7.16424937 9.49356702 4.1166404 ]\n",
      "0\n",
      "0\n",
      "[0. 0. 0. 0. 0.]\n",
      "your volatility is 0\n",
      "Your Appraisal Ratio is 0\n",
      "So your optimal portfolio with market neutral exposure is \n",
      " 0.0 in SMB, 0.0 in HML, 0.0 in RMW, 0.0 in CMA, 0.0 in MOM, and 0 in Mkt-RF\n"
     ]
    }
   ],
   "source": [
    "# The zeros below need to be replaced by the actual values!\n",
    "\n",
    "VolTarget=0.3/12**0.5 # making it monthly as the data\n",
    "W=Alpha@np.linalg.inv(Sigma_e)\n",
    "print(W)\n",
    "# optimal RELATIVE weights, need to calibrate the volatility\n",
    "# compute the variance of the W portoflio\n",
    "VarW=0\n",
    "print(VarW)\n",
    "#adjusting the weights to meet the volatility target\n",
    "w= 0\n",
    "print(w)\n",
    "# Ww is our final weights with the volatility target\n",
    "Ww=w*W\n",
    "# final weights\n",
    "print(Ww)\n",
    "\n",
    "# check vol (must be 30%)\n",
    "\n",
    "vol=0\n",
    "\n",
    "# market exposure\n",
    "\n",
    "Portfolio_beta=0\n",
    "\n",
    "# amount to buy in the market to hedge it completely\n",
    "h=-Portfolio_beta\n",
    "\n",
    "AppraisalRatio=0\n",
    "\n",
    "print(f\"your volatility is {vol}\")\n",
    "print(f\"Your Appraisal Ratio is {AppraisalRatio}\")\n",
    "print(f\"So your optimal portfolio with market neutral exposure is \\n {Ww[0]} in SMB, {Ww[1]} in HML, {Ww[2]} in RMW, {Ww[3]} in CMA, {Ww[4]} in MOM, and {h} in Mkt-RF\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b55be0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your volatility is 0\n",
      "So your optimal portfolio with market neutral exposure is \n",
      " 0.0 in SMB, 0.0 in HML, 0.0 in RMW, 0.0 in CMA, 0.0 in MOM, and 0 in Mkt-RF\n",
      "Your Appraisal Ratio is 0\n"
     ]
    }
   ],
   "source": [
    "# lets wrap it up in a function\n",
    "\n",
    "def sizing(W,Alpha,Sigma_e,Beta,VolTarget=0.3/12**0.5):\n",
    "    # need to replace the zeros below with the actual values!\n",
    "    VarW=0\n",
    "    #adjusting the weights to meet the volatility target\n",
    "    w= 0\n",
    "    Ww=w*W\n",
    "    vol=0\n",
    "    print(f\"your volatility is {vol}\")\n",
    "    Portfolio_beta=0\n",
    "    h=-Portfolio_beta\n",
    "    AppraisalRatio=0\n",
    "    print(f\"So your optimal portfolio with market neutral exposure is \\n {Ww[0]} in SMB, {Ww[1]} in HML, {Ww[2]} in RMW, {Ww[3]} in CMA, {Ww[4]} in MOM, and {h} in Mkt-RF\")\n",
    "    print(f\"Your Appraisal Ratio is {AppraisalRatio}\")\n",
    "    return AppraisalRatio, Ww\n",
    "\n",
    "W=Alpha@np.linalg.inv(Sigma_e)\n",
    "sr_alpha,W_alpha=sizing(W,Alpha,Sigma_e,Beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2fd1af",
   "metadata": {},
   "source": [
    "1/N rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10eda71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your volatility is 0\n",
      "So your optimal portfolio with market neutral exposure is \n",
      " 0.0 in SMB, 0.0 in HML, 0.0 in RMW, 0.0 in CMA, 0.0 in MOM, and 0 in Mkt-RF\n",
      "Your Appraisal Ratio is 0\n"
     ]
    }
   ],
   "source": [
    "W=np.ones(5)/5\n",
    "sr_alpha,W_alpha=sizing(W,Alpha,Sigma_e,Beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ecc64b",
   "metadata": {},
   "source": [
    "Proportional rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06de6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your volatility is 0\n",
      "So your optimal portfolio with market neutral exposure is \n",
      " 0.0 in SMB, 0.0 in HML, 0.0 in RMW, 0.0 in CMA, 0.0 in MOM, and 0 in Mkt-RF\n",
      "Your Appraisal Ratio is 0\n"
     ]
    }
   ],
   "source": [
    "W=Alpha\n",
    "sr_alpha,W_alpha=sizing(W,Alpha,Sigma_e,Beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb62afb",
   "metadata": {},
   "source": [
    "Risky Parity rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac36c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your volatility is 0\n",
      "So your optimal portfolio with market neutral exposure is \n",
      " 0.0 in SMB, 0.0 in HML, 0.0 in RMW, 0.0 in CMA, 0.0 in MOM, and 0 in Mkt-RF\n",
      "Your Appraisal Ratio is 0\n"
     ]
    }
   ],
   "source": [
    "W=np.ones(5)@np.linalg.inv(Sigma_e**0.5)\n",
    "sr_alpha,W_alpha=sizing(W,Alpha,Sigma_e,Beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00ce814",
   "metadata": {},
   "source": [
    "Minimum Variance rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db75e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your volatility is 0\n",
      "So your optimal portfolio with market neutral exposure is \n",
      " 0.0 in SMB, 0.0 in HML, 0.0 in RMW, 0.0 in CMA, 0.0 in MOM, and 0 in Mkt-RF\n",
      "Your Appraisal Ratio is 0\n"
     ]
    }
   ],
   "source": [
    "W=np.ones(5)@np.linalg.inv(Sigma_e)\n",
    "sr_alpha,W_alpha=sizing(W,Alpha,Sigma_e,Beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5bb0bd",
   "metadata": {},
   "source": [
    "Variance Shrinkage rule\n",
    "\n",
    "\n",
    "- I will shrink them to the the average vol\n",
    "- I will shrink by 50%\n",
    "- The idea makes sense when you think the different assets are kind of similar so you think a good chunk of the sample variation is noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7f7b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your volatility is 0\n",
      "So your optimal portfolio with market neutral exposure is \n",
      " 0.0 in SMB, 0.0 in HML, 0.0 in RMW, 0.0 in CMA, 0.0 in MOM, and 0 in Mkt-RF\n",
      "Your Appraisal Ratio is 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alan.moreira\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\alan.moreira\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "tau=0.5\n",
    "sigma_alpha=np.mean(Alpha_se)\n",
    "Sigma=Sigma_e*(1-tau)+tau*np.eye(5)*np.mean(np.diag(Sigma_e))\n",
    "W=Alpha@np.linalg.inv(Sigma)\n",
    "sr_alpha,W_alpha=sizing(W,Alpha,Sigma_e,Beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f673257a",
   "metadata": {},
   "source": [
    "## Portfolios and factor models\n",
    "\n",
    "\n",
    "Say you have\n",
    "\n",
    "- a vector of asset excess returns R (N by 1),\n",
    "- weights W (N by 1),\n",
    "- factor loadings B (N by 1),\n",
    "- alphas A (N by 1),\n",
    "- residuals U (N by 1)\n",
    "- f is a scalar excess return factor\n",
    "\n",
    "then I can write my portfolio return as\n",
    "\n",
    "$$r_p=W.T @R=W.T @(A+Bf+U)$$\n",
    "\n",
    "Because $E[U]=0$, Thus it follows that\n",
    "\n",
    "$$E[r_p]=W.T @(A+BE[f])$$\n",
    "\n",
    "and\n",
    "\n",
    "$$Var[r_p]=Var(W.T @(A+Bf+U))=(W.T@B)@Var(f)@(W.T@B).T+W.T@Var(U)@W$$\n",
    "\n",
    "simplifying a little bit we get\n",
    "\n",
    "$$Var[r_p]=Var(W.T @(A+Bf+U))=(W.T@B)@(B.T@W)*Var(f)+W.T@Var(U)@W$$\n",
    "\n",
    "-This is the total portfolio variance, What is the portfolio factor risk?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d9b58c",
   "metadata": {},
   "source": [
    "\n",
    "## Risk Profile Analysis\n",
    "\n",
    "\n",
    "- What each asset is contributing to my overall portfolio risk?\n",
    "\n",
    "- what is the asset that at the margin contributes the most?\n",
    "\n",
    "- If I want to reduce my factor risk, what is the asset that would reduce it most efficiently?\n",
    "\n",
    "\n",
    "\n",
    "That is, if I were to increase/decrease the position in an asset by a little bit  asset how much my risk would change?\n",
    "\n",
    "$$2B@(B.T@W)Var(f)+2Var(U)@W$$\n",
    "\n",
    "This is a vector where each entry tells you the increase in risk produced my a marginal increase in each asset\n",
    "\n",
    "So the highest number in this vector tell us what is the asset that allows us to achieve the highest risk reduction per change in position\n",
    "\n",
    "> Question: How do you find the asset that would lead to highest reduction in factor risk?\n",
    "\n",
    "\n",
    ">If you want understand where the formular come from, you need to know a bit of calculus because to answer these questions we differentiate the risk contribution by the weight W.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70290522",
   "metadata": {},
   "source": [
    "Lets apply this to our problem\n",
    "\n",
    "I will estiamte a signle factor model here so we cna use realistic numbers, but we will dsicuss this step next class.\n",
    "\n",
    "For now just think this as blackbox that is giving you numbers for B, Var(f), Var(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2895a364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "def factor_model_estimation(df):\n",
    "  \"\"\"\n",
    "  Runs a regression of each asset in df on the market column, storing the slope coefficients,\n",
    "  the variance of the market, and the variance of the residuals in a diagonal matrix.\n",
    "\n",
    "  Args:\n",
    "    df: A pandas DataFrame containing asset returns, with a 'MKT' column for the market returns.\n",
    "\n",
    "  Returns:\n",
    "    A tuple containing:\n",
    "      - betas: A pandas DataFrame with slope coefficients for each asset.\n",
    "      - market_variance: The variance of the market returns.\n",
    "      - residual_variance_matrix: A diagonal matrix containing the variance of the residuals for each asset.\n",
    "  \"\"\"\n",
    "\n",
    "  betas = pd.DataFrame()\n",
    "  residual_variances = []\n",
    "\n",
    "  for asset in df.columns:\n",
    "\n",
    "    # Run the regression\n",
    "    X = df['MKT']\n",
    "    Y = df[asset]\n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(Y, X).fit()\n",
    "\n",
    "    # Store the beta (slope coefficient)\n",
    "    betas.loc[asset, 'beta'] = model.params['MKT']\n",
    "\n",
    "    # Store the residual variance\n",
    "    residual_variances.append(model.resid.var())\n",
    "\n",
    "  market_variance = df['MKT'].var()\n",
    "  residual_variance_matrix = np.diag(residual_variances)\n",
    "  betas=betas.to_numpy()\n",
    "  betas.shape=(5,1)\n",
    "\n",
    "  return betas, market_variance, residual_variance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ab0466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.        ]\n",
      " [0.05671103]\n",
      " [0.66599021]\n",
      " [0.64903334]\n",
      " [0.09602475]]\n",
      "0.0019496947245542036\n",
      "[[9.30297741e-35 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 1.22233384e-03 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 2.68475481e-03 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.36361424e-03\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  3.88658814e-04]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "B, Varf, VarU=factor_model_estimation(Data)\n",
    "print(B)\n",
    "print(Varf)\n",
    "print(VarU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383dece0",
   "metadata": {},
   "source": [
    "Now lets implement this formula\n",
    "\n",
    "$$2B@(B.T@W)Var(f)+2Var(U)@W$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1de9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "W=np.ones((5,1))/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee8bb50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00192455],\n",
       "       [0.00059808],\n",
       "       [0.00235563],\n",
       "       [0.00179454],\n",
       "       [0.00034027]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "2*B @ (B.T @ W) * Varf + 2 * VarU @ W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbfce14",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19fb02b0",
   "metadata": {},
   "source": [
    "## The rise of \"pod shops\"\n",
    "\n",
    "\n",
    "Hedge funds used to be associated with Principal traders\n",
    "\n",
    "- George Soros\n",
    "- Julian Robertson\n",
    "- David Tepper\n",
    "- Paul Tudor Jones\n",
    "- Steve Cohen\n",
    "- John Paulson\n",
    "\n",
    "\n",
    "Now we have the rise of Citadel and Millenium among a few others\n",
    "\n",
    "Pod shops are organized totally different\n",
    "\n",
    "Idea generation are done at the pod level in groups of 5-10 people\n",
    "\n",
    "Capital is allocated to the pod by the hedge fund principal, who monitors exposures and hedges residual factor risk\n",
    "\n",
    "Why pod traders work for Citadel and not choose to manage their own fund?\n",
    "\n",
    "**A calculation**\n",
    "\n",
    "Consider a pod shop with N pods with SR sr and total vol $\\sigma$, what is their alpha? What is their SR?\n",
    "\n",
    "\n",
    "$$E[\\sum x^*_ir_i]=E[\\sum \\frac{\\alpha_i}{\\sigma^2_i}\\alpha_i]=N*sr^2$$\n",
    "\n",
    "\n",
    "$$Var[\\sum x^*_ir_i]=Var[\\sum \\frac{\\alpha_i}{\\sigma^2_i}\\epsilon_i]=\\sum (\\frac{\\alpha_i}{\\sigma^2_i})^2Var[\\epsilon_i]$$\n",
    "\n",
    "\n",
    "$$Var[\\sum x^*_ir_i]=\\sum (\\frac{\\alpha_i}{\\sigma^2_i})^2\\sigma^2_i=N sr^2$$\n",
    "\n",
    "so the Sharpe Ratio of the pool of pods is \n",
    "\n",
    "$$sr_{pool}=\\frac{Nsr_{pod}^2}{\\sqrt{N}sr_{pod}}=\\sqrt{N}sr_{pod}$$\n",
    "\n",
    "The Sharpe ratio of the pool of pods grows with the number of pods\n",
    "\n",
    "This means that if managed individually, the average pod will accumulate wealth at rate $\\sigma*sr$\n",
    "\n",
    "But if managed in a pool structure, the average pod will accumulate as $\\sqrt{N}\\sigma*sr$\n",
    "\n",
    "Finding a new uncorrelated idea is super valuable!\n",
    "\n",
    "The marginal change in cash flows are\n",
    "\n",
    "\n",
    "$$\\frac{\\sigma*sr_{pool}}{N}$$\n",
    "\n",
    "- decreases in the number of pods\n",
    "- But always positive--if indeed similar SR and uncorrelated!\n",
    "\n",
    "Note that this calculation under-estimates the value of the pod structure\n",
    "\n",
    " - The higher your Sharpe ratio, the more volatility you can take without bearing significant risk of loss (wealth just grows too quickly)\n",
    "\n",
    "But it also not realistic\n",
    "  - Your marginal pod is likely to be worse or correlated with the others\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
