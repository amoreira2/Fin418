{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "suspected-knitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcd2dd3",
   "metadata": {},
   "source": [
    "# Machine Learning \n",
    "\n",
    "The basic issue in finance is that we want to know how expected returns move around, but we only observe realized returns\n",
    "\n",
    "We can compile lots and lots of information/data about different assets\n",
    "\n",
    "We saw how to run OLS regression of returns on a large set of characteristics ( I think it was 30)\n",
    "\n",
    "But we didn;t even think interactions among them--say the value characteristic might have different information for returns for small vs big stocks--considering all these interactions would leads us to estimate 900 coefficients. And of course there are potentially many more characteristics and their lags that could be informative for expected returns and co-movement\n",
    "\n",
    "You can see that very quickly you run out of data\n",
    "\n",
    "Here where recent advances in machine learning can be super useful\n",
    "\n",
    "In the end of the day we want to estimate a function F that maps observed characteristics in future returns\n",
    "\n",
    "\n",
    "$$R_{t+1}=F(X_t)$$\n",
    "\n",
    "This function can be linear\n",
    "\n",
    "$$R_{t+1}=BX_t$$\n",
    "\n",
    "\n",
    " or linear in the interactions\n",
    "\n",
    " $$R_{t+1}=BX_t+C X_t ⊗ X_t$$\n",
    "\n",
    " Or have even higher order or non-linear relationships\n",
    "\n",
    " Here where the tools if machine learning can be useful to us\n",
    "\n",
    " We will now discuss a few of the most used methods\n",
    "\n",
    "\n",
    "- **Lasso Regression** (L1 regularization)\n",
    "- **Neural Network Regression** (customizable number of layers)\n",
    "- **Random Forest Regression**\n",
    "- **Gradient Boosted Regression Trees (GBRT)**\n",
    "- **Elastic Net Regression** (combination of L1 and L2 regularization)\n",
    "\n",
    "We will apply those to our data set\n",
    "\n",
    "We will have a training/estimation   sample (1972-1992),  a tuning sample (1992-2002), and a test sample (2002-2016)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caad5f0",
   "metadata": {},
   "source": [
    "### 1. **Lasso Regression**\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) regression is a linear regression model with L1 regularization. It minimizes the following objective:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta} \\left( \\frac{1}{2n} \\sum_{i=1}^n (y_i - X_i^\\top \\beta)^2 + \\alpha \\|\\beta\\|_1 \\right)\n",
    "$$\n",
    "\n",
    "- **Key Characteristics**:\n",
    "  - Shrinks some coefficients to exactly zero, effectively performing feature selection.\n",
    "  - Useful for sparse models where only a subset of predictors are important.\n",
    "  - Struggles with multicollinearity, as it tends to arbitrarily select one among correlated predictors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdfb702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming df is your dataframe\n",
    "# Extract Y (excess return) and X (characteristics)\n",
    "X = df.iloc[:, 3:]  # Characteristics (columns after the first 3)\n",
    "Y = df.iloc[:, 2]   # Excess return (3rd column)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "# Perform Lasso regression\n",
    "lasso = Lasso(alpha=0.1)  # You can adjust the alpha (regularization strength)\n",
    "lasso.fit(X_train, Y_train)\n",
    "\n",
    "# Coefficients and intercept\n",
    "print(\"Lasso Coefficients:\", lasso.coef_)\n",
    "print(\"Intercept:\", lasso.intercept_)\n",
    "\n",
    "# Optional: Evaluate on test data\n",
    "score = lasso.score(X_test, Y_test)\n",
    "print(\"R^2 on Test Data:\", score)\n",
    "\n",
    "# Optional: Predicted values\n",
    "Y_pred = lasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201021a3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### 2. **Neural Network Regression**\n",
    "A neural network is a flexible, non-linear model that uses layers of neurons to approximate complex relationships between inputs (\\( X \\)) and outputs (\\( Y \\)). The simplest form of a feedforward neural network can be expressed as:\n",
    "\n",
    "\\[\n",
    "\\hat{y} = f(W^{[L]} \\sigma(W^{[L-1]} \\dots \\sigma(W^{[1]} X + b^{[1]}) + b^{[L-1]}) + b^{[L]}\n",
    "\\]\n",
    "\n",
    "- **Key Characteristics**:\n",
    "  - Consists of an input layer, hidden layers, and an output layer.\n",
    "  - Activation functions (\\( \\sigma \\), e.g., ReLU or sigmoid) introduce non-linearity.\n",
    "  - The number of layers and neurons can be tuned to fit data complexity.\n",
    "  - Requires careful tuning of hyperparameters (e.g., learning rate, number of layers, epochs).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Random Forest Regression**\n",
    "Random Forest is an ensemble method that combines multiple decision trees to make predictions. Each tree is trained on a bootstrap sample of the data, and predictions are averaged:\n",
    "\n",
    "\\[\n",
    "\\hat{y} = \\frac{1}{T} \\sum_{t=1}^T h_t(X)\n",
    "\\]\n",
    "\n",
    "Where \\( h_t(X) \\) is the prediction of the \\( t \\)-th tree.\n",
    "\n",
    "- **Key Characteristics**:\n",
    "  - Reduces overfitting by averaging predictions across trees.\n",
    "  - Handles non-linear relationships and interactions between features well.\n",
    "  - Relatively robust to noisy data and outliers.\n",
    "  - Does not extrapolate beyond the range of the training data.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Gradient Boosted Regression Trees (GBRT)**\n",
    "GBRT is an ensemble technique that builds trees sequentially, where each tree corrects the errors of the previous one. The prediction is updated iteratively:\n",
    "\n",
    "\\[\n",
    "\\hat{y}_t(X) = \\hat{y}_{t-1}(X) + \\nu \\cdot g_t(X)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( g_t(X) \\): Gradient of the loss function with respect to predictions.\n",
    "- \\( \\nu \\): Learning rate, controlling the contribution of each tree.\n",
    "\n",
    "- **Key Characteristics**:\n",
    "  - Optimizes a differentiable loss function (e.g., squared error for regression).\n",
    "  - Can capture complex, non-linear patterns in the data.\n",
    "  - Requires careful tuning of hyperparameters (e.g., learning rate, number of trees, maximum tree depth).\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Elastic Net Regression**\n",
    "Elastic Net combines L1 (Lasso) and L2 (Ridge) regularization to balance feature selection and multicollinearity handling. The objective function is:\n",
    "\n",
    "\\[\n",
    "\\min_{\\beta} \\left( \\frac{1}{2n} \\sum_{i=1}^n (y_i - X_i^\\top \\beta)^2 + \\alpha_1 \\|\\beta\\|_1 + \\alpha_2 \\|\\beta\\|_2^2 \\right)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( \\|\\beta\\|_1 \\): Lasso penalty encourages sparsity.\n",
    "- \\( \\|\\beta\\|_2^2 \\): Ridge penalty shrinks coefficients to reduce multicollinearity.\n",
    "\n",
    "- **Key Characteristics**:\n",
    "  - Balances Lasso's feature selection and Ridge's stability with correlated predictors.\n",
    "  - Controlled by two hyperparameters:\n",
    "    - \\( \\alpha \\): Overall regularization strength.\n",
    "    - \\( \\rho \\) (mixing ratio): Balance between L1 and L2 penalties.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| **Model**                  | **Type**       | **Key Strengths**                                        | **Limitations**                            |\n",
    "|----------------------------|----------------|----------------------------------------------------------|--------------------------------------------|\n",
    "| Lasso Regression           | Linear         | Feature selection, interpretable coefficients           | Struggles with multicollinearity           |\n",
    "| Neural Network Regression   | Non-linear     | Flexible, captures complex patterns                     | Requires significant tuning and data       |\n",
    "| Random Forest Regression    | Non-linear     | Robust to overfitting, handles feature interactions well | Computationally expensive for large data   |\n",
    "| GBRT                       | Non-linear     | Accurate, optimizes for specific loss functions         | Sensitive to hyperparameters, overfitting  |\n",
    "| Elastic Net Regression      | Linear         | Handles multicollinearity, balances selection & stability | Can be slower than Ridge or Lasso          |\n",
    "\n",
    "Let me know if you’d like additional detail or comparisons!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
